{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic scientific python imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook \n",
    "# Spikeinterface imports (could do this cleaner, oh well)\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.exporters as sxp\n",
    "import spikeinterface.widgets as sw\n",
    "import spikeinterface.full as si\n",
    "import probeinterface as pi\n",
    "import spikeinterface.curation as scur\n",
    "\n",
    "# import spikeinterface_gui\n",
    "# Core python imports\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## If reading series of intan recordings\n",
    "rec_name = 'poke1_230520_153135'\n",
    "path_to_folder = Path('C:/Users/lwood39/Documents/VNCMP/2023-05-20/' + rec_name)\n",
    "dircontents = os.listdir(path_to_folder)\n",
    "file_names = [x for x in dircontents if '.rhd' in x]\n",
    "recording_list = []\n",
    "for file in file_names:\n",
    "    path_to_file = os.path.join(path_to_folder, file)\n",
    "    recording_list.append(se.IntanRecordingExtractor(path_to_file, stream_id='0'))\n",
    "recording = si.concatenate_recordings(recording_list)\n",
    "display(recording)\n",
    "\n",
    "# Uncomment to grab just a section of a recording\n",
    "# recording = recording.frame_slice(start_frame=0, end_frame=int(231*30000))\n",
    "\n",
    "## If reading open ephys recording session\n",
    "# path_to_folder = Path('G:/SponbergLab/Data/Leo_2023-03-29_15-03-40/Record Node 103/experiment1')\n",
    "# # path_to_folder = Path('G:/VNCMP/20230308/2023-03-08_13-08-30/Record Node 104')\n",
    "# recording = se.read_openephys(path_to_folder, block_index=0, stream_id='0')\n",
    "# recording = si.SelectSegmentRecording(recording, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove analog input channels if present, not needed for spike sorting\n",
    "if any('ADC' in s for s in recording.get_channel_ids()):  \n",
    "    recording = recording.remove_channels([x for x in recording.get_channel_ids() if 'ADC' in x])\n",
    "recording.get_channel_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = pi.read_probeinterface('A32_A1x32-Poly5-6mm-35s-100.json')\n",
    "pi.plotting.plot_probe_group(probe, with_channel_index=True, with_device_index=True)\n",
    "# recording.set_probegroup(probe)\n",
    "recording.set_probe(probe.probes[0], in_place=True)\n",
    "recording.get_probe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a cache of filtered data and cache of raw data\n",
    "recording_cache_filter = si.bandpass_filter(recording, freq_min=350, freq_max=5000).save(format='binary', n_jobs=8, chunk_duration='10s')\n",
    "recording_cache_raw = recording.save(format='binary', n_jobs=8, chunk_duration='10s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Sorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.available_sorters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorter = 'mountainsort5'\n",
    "print(ss.get_default_sorter_params(sorter))\n",
    "ss.get_sorter_params_description(sorter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = ss.get_default_sorter_params(sorter)\n",
    "params['scheme'] = '2'\n",
    "params['detect_sign'] = 1\n",
    "params['detect_threshold'] = 5.5\n",
    "params['freq_min'] = 350\n",
    "params['freq_max'] = 7000\n",
    "params['npca_per_subdivision'] = 15\n",
    "params['scheme2_phase1_detect_channel_radius'] = 50\n",
    "params['scheme2_training_duration_sec'] = 800\n",
    "params['scheme2_max_num_snippets_per_training_batch'] = 10000\n",
    "params['scheme3_block_duration_sec'] = 500\n",
    "   \n",
    "tic = time.perf_counter()\n",
    "sort = ss.run_sorter(\n",
    "    sorter,\n",
    "    recording=recording_cache_raw,\n",
    "    output_folder=sorter,\n",
    "#     docker_image=\"spikeinterface/\" + sorter + \"-compiled-base:latest\", #<- use for kilosort\n",
    "    verbose=False,\n",
    "    **params)\n",
    "print(f'{time.perf_counter()-tic} seconds elapsed')\n",
    "Audio('notification-sound.wav', autoplay=True)\n",
    "\n",
    "print(sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "job_kwargs = dict(n_jobs=8, chunk_duration=\"1s\", progress_bar=True)\n",
    "\n",
    "# wave_extract = si.extract_waveforms(\n",
    "#     recording_cache_justfilter, \n",
    "#     sort, \n",
    "#     './waveforms',\n",
    "#     ms_before=2., ms_after=2.,\n",
    "#     max_spikes_per_unit=100000,\n",
    "#     overwrite=True, \n",
    "#     **job_kwargs)\n",
    "# # Find redundant units, perform extraction again with those removed\n",
    "# sort_no_redundant = scur.remove_redundant_units(wave_extract, align=True)\n",
    "wave_extract = si.extract_waveforms(\n",
    "    recording_cache_justfilter, \n",
    "#     sort_no_redundant,\n",
    "    sort,\n",
    "    './waveforms',\n",
    "    ms_before=2., ms_after=2.,\n",
    "    max_spikes_per_unit=100000,\n",
    "    overwrite=True, \n",
    "    **job_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phy GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phy_save_path = './phy_folder/' + sorter + '_' + rec_name\n",
    "sxp.export_to_phy(wave_extract, \n",
    "                  phy_save_path, \n",
    "                  remove_if_exists=True,\n",
    "                  **job_kwargs)\n",
    "\n",
    "# save record of params\n",
    "with open(phy_save_path+'/params_log.txt', 'w') as f: \n",
    "    for key, value in params.items(): \n",
    "        f.write('%s:%s\\n' % (key, value))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To run Phy, use cmd or Powershell, and do one of the following:\n",
    " \n",
    " 1. Run the command spit out by the cell above, often something like:\n",
    " \n",
    " ```phy template-gui  C:\\Users\\lwood39\\Documents\\AutoSpikeSort\\phy_folder\\mountainsort5_poke1_230520_153135\\params.py```\n",
    " \n",
    " \n",
    " 2. Navigate to phy_folder created by the above cell, then run phy command\n",
    " \n",
    " Example:\n",
    " \n",
    " ```cd C:/Users/lwood39/Documents/AutoSpikeSort/phy_folder_kilosort```\n",
    " \n",
    " ```phy template-gui params.py```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHY IS IT CRASHING? Some general notes\n",
    "\n",
    "I've noticed a few common patterns that will lead to crashes. Here are some notes on those, in no particular order.\n",
    "\n",
    "- **Kilosort is crashing. Stack trace has something related to a gpuArray error, and/or mentions something like a nan**\n",
    "\n",
    "  This almost always seems to be one of the batches of kilosort containing no detected spikes. Kilosort runs on independent \"batches\", sections of your data of a certain length. If one of those has no spikes, it seems to typically create a nan value that gums up the works. The easy solution is to either (a) Only feed in data that has spiking of some kind throughout, or (b) increase the batch size, with the 'NT' parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further plotting or exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot traces straight:\n",
    "# sw.plot_timeseries(\n",
    "#     recording_cache_raw, \n",
    "#     time_range=(69, 420), \n",
    "#     channel_ids=['0'], \n",
    "#     return_scaled=True)\n",
    "# plt.show()\n",
    "\n",
    "# To get traces of a specific channel from specific frames:\n",
    "# data = recording_cache_raw.get_traces(\n",
    "#     start_frame=your_start_frame_here, \n",
    "#     end_frame=your_end_frame_here,\n",
    "#     return_scaled=True, \n",
    "#     channel_ids=['3'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ea45a68df6c081ed75564725b0d139197db1d4205c00e9c49160b50bc65c42b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
